{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find GLIMDA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Run Statistics: --- \n",
      "\n",
      " Number of steps                                 : 936\n",
      " Number of function evaluations                  : 1441\n",
      " Number of Jacobian*vector evaluations           : 1520\n",
      " Number of function eval. due to Jacobian eval.  : 1441\n",
      " Number of error test failures                   : 13\n",
      " Number of nonlinear iterations                  : 1437\n",
      " Number of nonlinear convergence failures        : 0\n",
      "\n",
      "Solver options:\n",
      "\n",
      " Solver                   : CVode\n",
      " Linear multistep method  : BDF\n",
      " Nonlinear solver         : Newton\n",
      " Linear solver type       : SPGMR\n",
      " Maximal order            : 5\n",
      " Tolerances (absolute)    : 1e-06\n",
      " Tolerances (relative)    : 1e-06\n",
      "\n",
      "Simulation interval    : 0.0 - 3600.0 seconds.\n",
      "Elapsed simulation time: 0.6658561000000001 seconds.\n",
      "(38.75052236687746, 67.4877117296064, 104.96721691478905, 0.981166687251344)\n"
     ]
    }
   ],
   "source": [
    "from custom_sim_RL import ProcessSimulation\n",
    "\n",
    "trialname = 'RLtrial'\n",
    "sim = ProcessSimulation(trialname)\n",
    "\n",
    "runtime = 3600\n",
    "sim.setup_run(runtime_cryst=runtime)\n",
    "result = sim.output()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjper\\miniconda3\\envs\\PharmaPy\\lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\jjper\\miniconda3\\envs\\PharmaPy\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'temp_init' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 142\u001b[0m\n\u001b[0;32m    137\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(best_policy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_crystallization_policy.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    108\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(dist\u001b[38;5;241m.\u001b[39mlog_prob(action))\n\u001b[1;32m--> 109\u001b[0m     state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# reward only at end\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Discounted returns\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m, in \u001b[0;36mCrystallizationEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m temp_prog_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_program)\n\u001b[0;32m     48\u001b[0m sim \u001b[38;5;241m=\u001b[39m ProcessSimulation(trialname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrl_trial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_prog_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruntime_cryst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m D10, D50, D90, span \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39moutput()\n\u001b[0;32m     51\u001b[0m reward \u001b[38;5;241m=\u001b[39m D50 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m span  \u001b[38;5;66;03m# Custom reward function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjper\\Documents\\RESEARCH\\takeda\\RL\\custom_sim_RL.py:65\u001b[0m, in \u001b[0;36mProcessSimulation.setup_run\u001b[1;34m(self, c_in, temp_program, runtime_cryst)\u001b[0m\n\u001b[0;32m     59\u001b[0m kinetics \u001b[38;5;241m=\u001b[39m CrystKinetics(solub_cts, nucl_prim\u001b[38;5;241m=\u001b[39mprim, nucl_sec\u001b[38;5;241m=\u001b[39msec,\n\u001b[0;32m     60\u001b[0m                  growth\u001b[38;5;241m=\u001b[39mgrowth)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Setup Crystallizer\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Initial Liquid characteristics\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m conc_init \u001b[38;5;241m=\u001b[39m kinetics\u001b[38;5;241m.\u001b[39mget_solubility(\u001b[43mtemp_init\u001b[49m)  \u001b[38;5;66;03m# kg/m**3\u001b[39;00m\n\u001b[0;32m     66\u001b[0m conc_init \u001b[38;5;241m=\u001b[39m (conc_init, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# API conc and 0 for the solvent.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Define the liquid phase for the crystallizer\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'temp_init' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from gymnasium import Env, spaces\n",
    "from custom_sim_RL import ProcessSimulation  # My class\n",
    "\n",
    "### Custom Gym Environment for Crystallization ###\n",
    "class CrystallizationEnv(Env):\n",
    "    def __init__(self, sim_time=3600, step_time=300):\n",
    "        super(CrystallizationEnv, self).__init__()\n",
    "        self.sim_time = sim_time\n",
    "        self.step_time = step_time\n",
    "        self.n_steps = sim_time // step_time\n",
    "\n",
    "        self.temp_min = 280.0\n",
    "        self.temp_max = 330.0\n",
    "        self.temp_rate = 0.5 / 60  # K/s\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)  # 0: cool, 1: hold, 2: heat\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, self.temp_min]),\n",
    "            high=np.array([self.sim_time, self.temp_max]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.temp_slope -= self.temp_rate\n",
    "        elif action == 2:\n",
    "            self.temp_slope += self.temp_rate\n",
    "\n",
    "        self.t += self.step_time\n",
    "        self.time_points.append(self.t)\n",
    "\n",
    "        new_temp = self.temp_program[-1][1] + self.temp_slope * self.step_time\n",
    "        new_temp = np.clip(new_temp, self.temp_min, self.temp_max)\n",
    "        self.temp_program.append([self.t, new_temp])\n",
    "\n",
    "        done = self.t >= self.sim_time\n",
    "        reward = 0.0\n",
    "\n",
    "        if done:\n",
    "            temp_prog_np = np.array(self.temp_program)\n",
    "            sim = ProcessSimulation(trialname=\"rl_trial\")\n",
    "            sim.setup_run(temp_program=temp_prog_np, runtime_cryst=self.sim_time)\n",
    "            D10, D50, D90, span = sim.output()\n",
    "            reward = D50 - 2.0 * span  # Custom reward function\n",
    "            obs = np.array([self.t, new_temp], dtype=np.float32)\n",
    "            return obs, reward, True, False, {\"D50\": D50, \"span\": span}\n",
    "\n",
    "        obs = np.array([self.t, new_temp], dtype=np.float32)\n",
    "        return obs, reward, False, False, {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.t = 0.0\n",
    "        self.temp_slope = 0.0\n",
    "        self.temp_program = [[0.0, 323.15]]\n",
    "        self.time_points = [0.0]\n",
    "        return np.array([self.t, 323.15], dtype=np.float32), {}\n",
    "\n",
    "\n",
    "\n",
    "### Policy Network ###\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "### Training Loop ###\n",
    "def train():\n",
    "    env = CrystallizationEnv(sim_time=1800, step_time=300)  # Use shorter runs for faster training\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "\n",
    "    policy = PolicyNetwork(obs_dim, act_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    gamma = 0.99\n",
    "\n",
    "    best_reward = -np.inf\n",
    "    best_policy = None\n",
    "\n",
    "    for episode in range(50):  # Start with 50 episodes for quick iteration\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            probs = policy(state_tensor)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            state, reward, done, truncated, info = env.step(action.item())\n",
    "            rewards.append(reward if done else 0.0)  # reward only at end\n",
    "\n",
    "        # Discounted returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Policy gradient update\n",
    "        loss = sum(-log_prob * Gt for log_prob, Gt in zip(log_probs, returns))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_reward = sum(rewards)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, D50: {info.get('D50', 0):.1f}, Span: {info.get('span', 0):.2f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_policy = policy.state_dict()\n",
    "\n",
    "    print(\"Training complete. Best reward:\", best_reward)\n",
    "    if best_policy:\n",
    "        torch.save(best_policy, \"best_crystallization_policy.pth\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PharmaPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
